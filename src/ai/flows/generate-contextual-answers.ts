'use server';

import {ai} from '@/ai/genkit';
import {z} from 'genkit';

const GenerateContextualAnswerInputSchema = z.object({
  query: z.string().describe('The user query to answer.'),
  relevantChunks: z.array(z.string()).describe('The relevant text chunks retrieved from the vector database.'),
});
export type GenerateContextualAnswerInput = z.infer<typeof GenerateContextualAnswerInputSchema>;

const GenerateContextualAnswerOutputSchema = z.object({
  answer: z.string().describe('The contextual answer generated by the LLM.'),
});
export type GenerateContextualAnswerOutput = z.infer<typeof GenerateContextualAnswerOutputSchema>;

export async function generateContextualAnswer(input: GenerateContextualAnswerInput): Promise<GenerateContextualAnswerOutput> {
  return generateContextualAnswerFlow(input);
}

const generateContextualAnswerFlow = ai.defineFlow(
  {
    name: 'generateContextualAnswerFlow',
    inputSchema: GenerateContextualAnswerInputSchema,
    outputSchema: GenerateContextualAnswerOutputSchema,
  },
  async ({ query, relevantChunks }) => {
    const promptText = `You are an AI assistant that answers questions based on the provided context.

    Context:
    ${relevantChunks.join('\n\n---\n\n')}

    Question: ${query}

    Answer:`;

    const llmResponse = await ai.generate({
        prompt: promptText,
    });

    const answer = llmResponse.output();
    if (answer === null || answer === undefined) {
        throw new Error("Failed to get a response from the model.");
    }

    return { answer };
  }
);
